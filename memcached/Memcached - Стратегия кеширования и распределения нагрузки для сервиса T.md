### Общие архитектурные соображения

1. **Цель**: Обеспечить обработку запросов на получение сроков доставки с P90 RT ≤ 20 мс и P99 RT ≤ 50 мс при пиковой нагрузке 210k rps, используя Memcached как основное хранилище кеша.
2. **Особенности**:
    - Вычисление сроков доставки занимает значительное время, поэтому кеш обязателен.
    - Нагрузка сильно зависит от популярных локаций (Москва — 40%, СПб — 20%).
    - В запросах участвует от 50 до 2000 складов, что требует эффективного хранения и быстрого доступа к данным.
    - Для запросов с 2000 складами используется mget для извлечения всех ключей одним вызовом.
3. **Расчеты**:
    - Максимальная нагрузка: 210,000 rps.
    - Размер данных на запрос: ключ (локация + склад) ~50 байт, значение (срок доставки) ~10 байт → ~60 байт/запись.
    - Общий объем кеша: 30,000 локаций × 150,000 складов × 60 байт = ~270 ГБ (без учета фрагментации и служебных данных).
### Архитектура решения

#### 1. Выбор кластера Memcached

- **Размер кластера**:
    - Общий объем данных ~270 ГБ, с запасом на рост и фрагментацию — 400 ГБ.
    - Memcached не поддерживает репликацию на уровне самого сервиса, поэтому данные распределяются между нодами.
- **Количество инстансов**:
    - Используем 16 нод Memcached (по 25 ГБ RAM на ноду → 400 ГБ суммарно).
    - CPU: 4 vCPU на ноду (для обработки 210k rps минималка).
    - RAM: 32 ГБ на ноду (25 ГБ под кеш, остальное под ОС и overhead).
- **Обоснование**:
    - 210,000 rps ÷ 16 нод = ~13,125 rps на ноду, что Memcached легко выдерживает при 4 vCPU.
    - 400 ГБ RAM покрывает полный кеш с запасом на пиковые нагрузки.

#### 2. Распределение данных по нодам

- **Механизм**: Consistent Hashing (с использованием библиотеки, например, libketama).
    - Каждый ключ (формат: locationId:warehouseId) хешируется и распределяется по нодам.
    - Преимущество: минимизация перемещения данных при добавлении/удалении нод.
- **Репликация**:
    - Memcached не поддерживает встроенную репликацию. Для отказоустойчивости используем клиентскую логику:
        - Дублирование данных на уровне приложения (запись в 2 ноды для горячих ключей).
- **Распределение по ключам**:
    - Ключ: locationId:warehouseId (например, moscow:12345).
    - Горячие локации (Москва, СПб) распределяются равномерно благодаря хешированию.

#### 3. Управление TTL

- **Стратегия**:
    - Базовый TTL: 24 часа (сроки доставки редко меняются чаще).
    - Обновление: Асинхронное авто-обновление через фоновый процесс:
        - Сервис вычисляет сроки доставки для устаревших или отсутствующих ключей и обновляет кеш.
        - Частота: каждые 6 часов для всех данных, чаще (каждые 30 минут) для горячих ключей (Москва, СПб).
- **Вытеснение**:
    - Memcached использует LRU (Least Recently Used) для автоматического удаления старых данных при переполнении.

#### 4. Стратегия работы с горячими ключами

- **Проблема**: Москва (40% запросов = 84k rps) и СПб (20% = 42k rps) создают горячую нагрузку.
- **Решение**:
    - **Локальный кеш на клиенте**: Каждый клиентский инстанс (например, приложение на Go) держит in-memory кеш (LRU, 1 ГБ) для топ-1000 популярных пар locationId:warehouseId.
    - **Дублирование**: Горячие ключи записываются в 2 ноды Memcached (основная + запасная).
    - **Шардирование**: Разделение Москвы и СПб на подгруппы (например, по районам), чтобы снизить концентрацию на одном ключе.

#### 5. Механизм отказоустойчивости

- **Recovery Strategy**:
    - При сбое ноды:
        - Клиент перенаправляет запросы к другим нодам (consistent hashing минимизирует потери).
        - Асинхронный бэкграунд-процесс восстанавливает данные на новой ноде из источника (например, БД).
    - Для горячих ключей: чтение из запасной ноды, если основная недоступна.
- **Тайм-ауты**: Клиентский тайм-аут на чтение — 5 мс, после чего запрос уходит в дефолтное значение (например, "срок неизвестен").

#### 6. Балансировка запросов

- **Слой балансировки**:
    - Прокси-сервер (например, Twemproxy с ограничением в 4096 макс соединений) между клиентами и Memcached:
        - Распределяет запросы по нодам.
        - Уменьшает количество соединений к Memcached.
    - Клиентская библиотека (например, gomemcache) с поддержкой consistent hashing.
- **Локальный кеш**:
    - Каждый клиент держит LRU-кеш (1 ГБ) для горячих данных, снижая нагрузку на Memcached на 20–30%.
- **Промежуточные слои**:
    - Не используем (добавляет latency), полагаемся на Twemproxy и клиентскую логику.

---

### Расчеты и обоснования

#### Объем данных

- Полный кеш: 30,000 × 150,000 × 60 байт = 270 ГБ.
- Горячие данные (Москва + СПб): 60% × 270 ГБ = ~162 ГБ.

#### Производительность

- Memcached выдерживает ~50k rps на ноду с 4 vCPU.
- 16 нод × 50k rps = 800k rps (запас в ~4 раз от пика 210k rps).
- Latency: Memcached дает ~1-2 мс на чтение, с Twemproxy и сетью — до 15 мс, что укладывается в P90 ≤ 40 мс.

#### Сеть

- Трафик: 210k rps × 60 байт = ~12.6 МБ/с на чтение + запись → ~25 МБ/с суммарно.
- 16 нод: ~1.6 МБ/с на ноду — сеть не перегружена.

### Блок-схема обработки запросов

```
Клиентский запрос (gRPC/HTTP)
    │ (валидация, <1 мс)
    ▼
Локальный кеш (LRU, 1 ГБ)
    │
    ├─Hit→ Ответ (<5 мс)
    │
    └─Miss→ Twemproxy
                │
                ▼ (consistent hashing)
Memcached кластер (16 нод, 4 vCPU, -c 4096, -t 4)
    │
    ├─Hit→ Ответ (<10 мс)
    │
    └─Miss→ Дефолтное значение (<15 мс)
                │
                ▼ (асинхронно)
Фоновый процесс → Вычисление сроков → Обновление Memcached
    │ (для горячих ключей: запись в 2 ноды)
```

### Ответы на дополнительные вопросы

#### Как справиться с пиковыми нагрузками без перегрева Memcached?

- Локальный кеш на клиентах снижает нагрузку на 20–30%.
- Дублирование горячих ключей на 2 ноды распределяет трафик.
- Авто-скейлинг: при превышении 80% RAM добавляем новые ноды Memcached (consistent hashing минимизирует перераспределение).

#### Как снизить нагрузку на сеть?

- Локальный кеш уменьшает количество запросов к Memcached.
- Батчинг: клиенты запрашивают данные для нескольких складов одним вызовом (mget).
- Сжатие ключей/значений (например, protobuf вместо plain text).

#### Какие метрики и алерты мониторить?

- **Метрики**:
    - Cache hit rate (цель: >90%).
    - Latency (P90, P99).
    - RPS на ноду Memcached.
    - CPU/RAM usage на нодах.
    - Network I/O.
- **Алерты**:
    - Cache hit rate < 85%.
    - P99 RT > 50 мс.
    - CPU > 80% или RAM > 90% на любой ноде.
    - Потеря ноды (доступность < 100%).

### Пояснения

- **Производительность**: Локальный кеш и Twemproxy обеспечивают P90 ≤ 20 мс и P99 ≤ 50 мс.
- **Отказоустойчивость**: Consistent hashing и дублирование горячих ключей минимизируют влияние сбоев.
- **Масштабируемость**: 16 нод с запасом по RAM и CPU легко справляются с 210k rps, с возможностью роста до 1M rps.